# √éle-de-France ‚Äì Annonces immobili√®res (SeLoger)

[![CI ‚Äì Scrape & Clean](https://github.com/kaoutarCHRAIM/PMN_Projet/actions/workflows/main.yml/badge.svg)](https://github.com/kaoutarCHRAIM/PMN_Projet/actions)

**App en ligne :** https://pmnprojet-aqcouv52qt3k9bamgwkxgz.streamlit.app/  
**Historique CI/CD :** https://github.com/kaoutarCHRAIM/PMN_Projet/actions  
**Donn√©es √† jour :** https://github.com/kaoutarCHRAIM/PMN_Projet/blob/master/data/cleaned_data.csv

## R√©sum√© du projet
Ce projet met en place un pipeline complet pour :
- Collecter des annonces immobili√®res (pages HTML sauvegard√©es ou spider Scrapy).
- Nettoyer et normaliser les donn√©es (prix, surface, ‚Ç¨/m¬≤, pi√®ces, code postal, coordonn√©es).
- Explorer les r√©sultats dans un tableau de bord Streamlit (carte + tableau).
- Automatiser la mise √† jour via GitHub Actions (CI/CD).

---

## 1. D√©p√¥t GitHub structur√©
Le d√©p√¥t est organis√© de mani√®re claire :

| Chemin                       | R√¥le                                               |
|------------------------------|----------------------------------------------------|
| `src/spider.py`              | Spider/Parser ‚Äî extraction depuis HTML (titre, prix, surface, pi√®ces, ville, CP, lat/lon, URL). |
| `src/parse_local_html.py`    | Variante pour parsing de fichiers HTML locaux.     |
| `src/cleaner.py`             | Nettoyage robuste (formats JSON vari√©s) + normalisation, g√©ocodage, filtre √éDF, export CSV. |
| `src/app.py`                 | Application Streamlit (filtres, carte pydeck, tableau num√©rot√©, liens). |
| `data/raw_data.json`         | Donn√©es brutes (sortie spider).                    |
| `data/cleaned_data.csv`      | Donn√©es nettoy√©es (entr√©e Streamlit).              |
| `.github/workflows/main.yml` | Pipeline CI/CD GitHub Actions.                     |
| `requirements.txt`           | D√©pendances Python.                                |

---

## 2. Pipeline CI/CD fonctionnel
Le fichier `.github/workflows/main.yml` assure l‚Äôautomatisation :
- **D√©clencheurs** : manuel (`workflow_dispatch`) + quotidien (cron 06:15 UTC).
- **√âtapes** : checkout ‚Üí setup Python 3.11 ‚Üí install deps ‚Üí run spider ‚Üí run cleaner ‚Üí check CSV ‚Üí commit conditionnel.
- **Robustesse** : `continue-on-error: true` pour √©viter l‚Äô√©chec global.
- **Condition de commit** : push uniquement si `cleaned_data.csv` contient au moins 1 ligne.

---

## 3. Tableau de bord Streamlit d√©ploy√©
L‚Äôapplication Streamlit permet :
- Des filtres (prix, surface, villes).
- Une carte pydeck avec points et labels k‚Ç¨.
- Un tableau interactif avec liens directs vers les annonces.

üëâ **Lien URL** : https://pmnprojet-aqcouv52qt3k9bamgwkxgz.streamlit.app/

---

## 4. Rapport de projet (README.md)
### ‚Ä¢ Le probl√®me abord√©
Comparer les annonces en √éle-de-France est chronophage. Ce projet centralise et standardise les infos cl√©s pour une exploration rapide.

### ‚Ä¢ L‚Äôarchitecture du pipeline (Scrapy ‚Üí Streamlit)
```
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ Pages HTML    ‚îÇ --> ‚îÇ Spider/Parser ‚îÇ --> ‚îÇ raw_data.json ‚îÇ --> ‚îÇ Cleaner     ‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                               ‚îÇ
                                                               v
                                                        cleaned_data.csv
                                                               ‚îÇ
                                                               v
                                                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                        ‚îÇ Streamlit App ‚îÇ
                                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

CI/CD GitHub Actions : planification quotidienne + commit conditionnel
```

### ‚Ä¢ Comment lancer le projet en local
1. Cr√©er un venv Python 3.11 et installer les d√©pendances : `pip install -r requirements.txt`.
2. D√©poser des fichiers HTML dans `data/html/`.
3. G√©n√©rer le brut : `python src/spider.py` ou `python src/parse_local_html.py`.
4. Nettoyer : `python src/cleaner.py` ‚Üí produit `data/cleaned_data.csv`.
5. Lancer le dashboard : `streamlit run src/app.py`.

### ‚Ä¢ D√©fis techniques & solutions
- **Formats JSON h√©t√©rog√®nes** : fonction `load_raw` tol√©rante.
- **Nettoyage CP** : regex stricte sur 5 chiffres.
- **Coordonn√©es manquantes** : g√©ocodage pgeocode.
- **Filtre √éDF** : bbox (lat: 48.0‚Äì49.3, lon: 1.45‚Äì3.57).
- **D√©duplication** : suppression doublons (url, title).
- **CI r√©siliente** : erreurs tol√©r√©es + commit conditionnel.

---

## 5. D√©pendances (versions)
- beautifulsoup4==4.12.3  
- lxml==5.2.2  
- pandas==2.2.2  
- python-dotenv==1.0.1  
- requests==2.32.3  
- streamlit==1.37.1  
- pydeck==0.9.1  
- scrapy==2.11.2  
- pgeocode==0.5.0

---

## 6. Annexe ‚Äî Workflow GitHub Actions
R√©sum√© de `.github/workflows/main.yml` :
- Checkout
- Setup Python 3.11
- Install deps (requirements + scrapy)
- Run spider ‚Üí `data/raw_data.json`
- Run cleaner ‚Üí `data/cleaned_data.csv`
- Check CSV > 1 ligne
- Commit & push si OK

---

## Auteurs
Projet r√©alis√© par :
- **Kaoutar Chraim**
- **Ouiddir Manel**

*Mast√®re 2 ‚Ä¢ Data Analyse (2024-2025)*
